{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3f0ba6",
   "metadata": {},
   "source": [
    "# Week 7-8: Deep Learning and AI\n",
    "Welcome to Week 7-8! We’re diving into Deep Learning and AI, a fascinating leap from traditional machine learning. This guide will take you through the essentials of deep learning, from basic neural networks to advanced architectures, with practical examples using Python. Let’s get started!\n",
    "## 1. What is Deep Learning?\n",
    "**Deep learning** is a subset of machine learning that uses neural networks with multiple layers (hence \"deep\") to model complex patterns in data. It’s incredibly powerful for tasks like:\n",
    "**Image Recognition:** Identifying objects in photos.\n",
    "\n",
    "**Natural Language Processing (NLP):** Understanding and generating text.\n",
    "\n",
    "**Game Playing:** Mastering games like Go or Chess.\n",
    "\n",
    "Unlike traditional machine learning, deep learning automatically learns features from raw data, reducing the need for manual feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08848164",
   "metadata": {},
   "source": [
    "## 2. Neural Networks: The Foundation\n",
    "### How Neural Networks Work\n",
    "**Neural networks** are inspired by the human brain, consisting of layers of interconnected nodes (neurons). Each connection has a weight that’s adjusted during training to improve predictions.\n",
    "#### Key Components\n",
    "**Input Layer:** Where data enters (e.g., pixel values of an image).\n",
    "\n",
    "**Hidden Layers:** Perform computations to extract features. More layers = deeper network.\n",
    "\n",
    "**Output Layer:** Produces the final prediction (e.g., a class label).\n",
    "\n",
    "#### Training Process\n",
    "**Forward Propagation:** Data passes through the network to generate predictions.\n",
    "\n",
    "**Loss Function:** Measures the error between predictions and actual values (e.g., Cross-Entropy Loss for classification).\n",
    "\n",
    "**Backpropagation:** Adjusts weights using gradients to minimize the loss.\n",
    "\n",
    "**Optimizer:** Updates weights (e.g., Adam, SGD).\n",
    "\n",
    "#### Activation Functions\n",
    "These introduce non-linearity, enabling the network to learn complex patterns:\n",
    "**ReLU (Rectified Linear Unit):** f(x) = max(0, x) – Fast and avoids vanishing gradients.\n",
    "\n",
    "**Sigmoid:** f(x) = 1 / (1 + e^-x) – Outputs 0 to 1, great for binary classification.\n",
    "\n",
    "**Softmax:** Converts outputs to probabilities, used for multi-class classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5910a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    layers.Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    layers.Dense(10, activation='softmax')  # Output layer for 10 digits\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96687f47",
   "metadata": {},
   "source": [
    "### Key Steps Explained\n",
    "1.\t**Data Preprocessing:** Normalize pixel values to [0, 1] for faster convergence.\n",
    "2.\t**Model Architecture:** A simple feedforward network with one hidden layer.\n",
    "3.\t**Compilation:** Choose an optimizer, loss function, and metrics.\n",
    "4.\t**Training:** Fit the model to the training data over multiple epochs.\n",
    "5.\t**Evaluation:** Test the model on unseen data to assess generalization.\n",
    "#### Tips\n",
    "•\t**Epochs:** Number of times the model sees the entire dataset. Too many can lead to overfitting.\n",
    "•\t**Batch Size:** Number of samples processed before updating weights (default is 32).\n",
    "•\t**Learning Rate:** Controls how much weights are adjusted; too high can cause instability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630d5db",
   "metadata": {},
   "source": [
    "This code builds a simple neural network to classify handwritten digits from the MNIST dataset. Here's what's happening:\n",
    "#### 1.\tData Loading and Preprocessing:\n",
    "o\tWe load the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits (0-9).\n",
    "o\tWe normalize the pixel values to be between 0 and 1.\n",
    "#### 2.\tModel Building:\n",
    "o\tWe use a Sequential model, which is a linear stack of layers.\n",
    "o\tThe Flatten layer converts the 2D images into 1D arrays.\n",
    "o\tThe Dense layers are fully connected layers. The first one has 128 neurons with ReLU activation, and the second one has 10 neurons (one for each digit) with softmax activation.\n",
    "#### 3.\tModel Compilation:\n",
    "o\tWe specify the optimizer (adam), the loss function (sparse_categorical_crossentropy for multi-class classification), and the metrics to track (accuracy).\n",
    "#### 4.\tModel Training:\n",
    "o\tWe train the model on the training data for 5 epochs (iterations over the entire dataset).\n",
    "#### 5.\tModel Evaluation:\n",
    "o\tWe evaluate the model's performance on the test data and print the accuracy.\n",
    "This is a basic example, but it illustrates the fundamental steps in building and training a neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea3a4b",
   "metadata": {},
   "source": [
    "## 3. Convolutional Neural Networks (CNNs): Image Processing\n",
    "#### What are CNNs?\n",
    "**CNNs** are specialized neural networks for processing grid-like data, such as images. They use convolutional layers to automatically learn spatial hierarchies of features, making them ideal for tasks like image classification and object detection.\n",
    "#### Key Components\n",
    "•\t**Convolutional Layers:** Apply filters to detect patterns (e.g., edges, textures).\n",
    "•\t**Pooling Layers:** Reduce spatial dimensions (e.g., max pooling) to decrease computation and prevent overfitting.\n",
    "•\t**Fully Connected Layers:** Perform classification based on features extracted by convolutional layers.\n",
    "**Example:** CNN for Image Classification\n",
    "Here’s a CNN for classifying CIFAR-10 images (10 classes like cats, dogs, etc.):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116614ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load and preprocess CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
    "\n",
    "# Build the CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d878b",
   "metadata": {},
   "source": [
    "### Tips:\n",
    "•\tIncrease epochs or filters for better accuracy.\n",
    "•\tAdd data augmentation (e.g., rotation) to improve generalization.\n",
    "\n",
    "### Key Tips\n",
    "•\t**Filters:** Increase the number of filters in deeper layers to capture more complex features.\n",
    "•\t**Pooling:** Use max pooling to retain the most important features.\n",
    "•\t**Data Augmentation:** Apply transformations (e.g., rotation, flipping) to increase dataset diversity.\n",
    "•\t**Use Case:** Image classification, object detection, facial recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e186d42",
   "metadata": {},
   "source": [
    "## 4. Recurrent Neural Networks (RNNs): Sequence Modeling\n",
    "### What are RNNs?\n",
    "**RNNs** are designed for sequential data (e.g., time series, text) by maintaining a \"memory\" of previous inputs through loops in the network. They’re ideal for tasks like language modeling or stock price prediction.\n",
    "### Key Variants\n",
    "•\t**Long Short-Term Memory (LSTM):** Addresses the vanishing gradient problem, allowing the network to learn long-term dependencies.\n",
    "•\t**Gated Recurrent Unit (GRU):** A simpler alternative to LSTM with similar performance.\n",
    "**Example:** LSTM for Text Classification\n",
    "Here’s an LSTM for sentiment analysis on the IMDB movie review dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e623d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Load and preprocess IMDB data\n",
    "max_features = 10000  # Vocabulary size\n",
    "maxlen = 500  # Max sequence length\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(max_features, 128),\n",
    "    layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train, epochs=3, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71a434",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "•\t**Embedding:** Converts words to dense vectors.\n",
    "•\t**Dropout:** Prevents overfitting by randomly disabling neurons\n",
    "### Key Tips\n",
    "•\t**Padding:** Use pad_sequences to ensure all inputs have the same length.\n",
    "•\t**Dropout:** Apply dropout to prevent overfitting in RNNs.\n",
    "•\t**Use Case:** Text classification, language translation, time series forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5dc88f",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning: Leveraging Pre-trained Models\n",
    "#### What is Transfer Learning?\n",
    "**Transfer learning** involves using a pre-trained model (trained on a large dataset like ImageNet) and fine-tuning it for a specific task. This is especially useful when you have limited data.\n",
    "#### How It Works\n",
    "•\t**Feature Extraction:** Use the pre-trained model’s layers to extract features, then train a new classifier on top.\n",
    "•\t**Fine-Tuning:** Unfreeze some layers of the pre-trained model and train them on your data for better performance.\n",
    "**Example:** Transfer Learning with VGG16\n",
    "Here’s how to use VGG16 for image classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc148f66",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load pre-trained VGG16 model (without the top layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Assuming 10 classes\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e86052",
   "metadata": {},
   "source": [
    "### Tips:\n",
    "•\tResize your images to 224x224 for VGG16.\n",
    "•\tUnfreeze some layers later for fine-tuning.\n",
    "### Key Tips\n",
    "•\t**Freeze Layers:** Prevent the pre-trained layers from updating during initial training.\n",
    "•\t**Fine-Tune:** Unfreeze some layers later for better accuracy.\n",
    "•\t**Use Case:** Image classification with small datasets, NLP tasks with BERT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfc1b98",
   "metadata": {},
   "source": [
    "## 6. Generative Models: Creating New Data\n",
    "#### What are Generative Models?\n",
    "**Generative models** learn to create new data that resembles the training data. They’re used for tasks like image generation, style transfer, and data augmentation.\n",
    "#### Key Types\n",
    "•\t**Generative Adversarial Networks (GANs):** Consist of a generator (creates fake data) and a discriminator (tries to distinguish real from fake). They compete, improving each other.\n",
    "•\t**Variational Autoencoders (VAEs):** Encode data into a latent space and decode it back, useful for generating new samples and anomaly detection.\n",
    "**Example: Simple GAN (Conceptual)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fc84a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pseudocode for a GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Train the GAN\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator on real and fake data\n",
    "    # Train generator to fool the discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558f5a5",
   "metadata": {},
   "source": [
    "### Key Tips\n",
    "•\t**Training Stability:** GANs can be tricky to train; use techniques like batch normalization and careful hyperparameter tuning.\n",
    "•\t**Use Case:** Image generation, data augmentation, art creation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
